{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade fastcore -q\n",
    "!pip install --upgrade fastai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9255b84daffc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcrashes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcrashes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "crashes = pd.read_csv('Train.csv')\n",
    "crashes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 6)\n",
    "cols = ['datetime', 'latitude', 'longitude']\n",
    "X = crashes[cols]\n",
    "kmeans.fit(X[X.columns[1:3]])\n",
    "X['cluster_labels'] = kmeans.fit_predict(X[X.columns[1:3]])\n",
    "centers = kmeans.cluster_centers_\n",
    "centroids = centers.copy()\n",
    "centers = torch.from_numpy(centers)\n",
    "centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambulance_loc = centers.clone()\n",
    "#ambulance_loc = torch.from_numpy(ambulance_loc)\n",
    "ambulance_loc.requires_grad_()\n",
    "crash_locations = tensor(crashes[['latitude', 'longitude']].values)\n",
    "#define loss function \n",
    "def loss_fn(crash_locations, ambulance_loc):\n",
    "  \"\"\"For each crash we find the dist to the closest ambulance, and return the mean of these dists.\"\"\"\n",
    "  # Dists to first ambulance\n",
    "  dists_split = crash_locations-ambulance_loc[0]\n",
    "  dists = (dists_split[:,0]**2 + dists_split[:,1]**2)**0.5\n",
    "  min_dists = dists\n",
    "  for i in range(1, 6):\n",
    "    # Update dists so they represent the dist to the closest ambulance\n",
    "    dists_split = crash_locations-ambulance_loc[i]\n",
    "    dists = (dists_split[:,0]**2 + dists_split[:,1]**2)**0.5\n",
    "    min_dists = torch.min(min_dists, dists)\n",
    "  \n",
    "  return min_dists.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_loss = loss_fn(crash_locations, ambulance_loc)\n",
    "computed_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the gradients can be computed as follows:\n",
    "computed_loss.backward()\n",
    "ambulance_loc.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the loss can be reduced based on inferences from the gradients above as follows: \n",
    "ambulance_loc.grad = None # Reset gradients\n",
    "lr=1e-2 # Pick LR\n",
    "for i in range(1000):\n",
    "  computed_loss = loss_fn(crash_locations, ambulance_loc) # Find loss\n",
    "  if i % 100 == 0: print('Loss:', computed_loss.item()) # Print loss\n",
    "  computed_loss.backward() # Calc grads\n",
    "  ambulance_loc.data -= lr * ambulance_loc.grad.data # Update locs\n",
    "  ambulance_loc.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot crashes and ambulance locations\n",
    "ax = crashes.plot(y='latitude', x='longitude', kind='scatter', figsize=(16, 10), label='crash locations')\n",
    "ax.scatter(centers[:,1].detach().numpy(), ambulance_loc[:,0].detach().numpy(), c='red', label='initial ambulance locs')\n",
    "ax.scatter(ambulance_loc[:,1].detach().numpy(), ambulance_loc[:,0].detach().numpy(), c='yellow', label='final ambulance locs')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing the SGD\n",
    "# Split for validation\n",
    "train_locations = crash_locations[:-1000]\n",
    "val_locations = crash_locations[-1000:]\n",
    "\n",
    "# Load crash locs from train into a dataloader\n",
    "dl = DataLoader(train_locations, batch_size=1000\n",
    "                , shuffle=True)\n",
    "\n",
    "# Set up ambulance locations\n",
    "ambulance_locations = centers\n",
    "ambulance_locations.requires_grad_()\n",
    "\n",
    "# Set vars\n",
    "lr=3e-3\n",
    "n_epochs = 70\n",
    "mom = 0.9\n",
    "# Store loss over time\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "   # Run through batches\n",
    "  for crashes in dl:\n",
    "    loss = loss_fn(crashes, ambulance_locations) # Find loss for this batch of crashes\n",
    "    loss.backward() # Calc grads\n",
    "    ambulance_locations.data -= lr * ambulance_locations.grad.data # Update locs\n",
    "    ambulance_locations.grad = None # Reset gradients for next step\n",
    "    train_losses.append(loss.item())\n",
    "    val_losses.append(loss_fn(val_locations, ambulance_locations).item()) # Can remove as this lows things down\n",
    "\n",
    "  # Print validation loss\n",
    "  print('Val loss:', loss_fn(val_locations, ambulance_locations).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambulance_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambulance_locations\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.cluster_labels.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.date_range('2019-01-01', '2020-01-01', freq='3h')\n",
    "submission = pd.DataFrame({'date':dates})\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambulance_locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def random_a_0_latitudes(initial_loc, final_loc, num): \n",
    "    res = [] \n",
    "  \n",
    "    for j in range(num): \n",
    "        res.append(random.uniform(initial_loc, final_loc)) \n",
    "  \n",
    "    return res\n",
    "A0_lat = random_a_0_latitudes(-1.2640,-1.2643, len(submission))\n",
    "A0_long = random_a_0_latitudes(36.7450, 36.7452, len(submission))\n",
    "A1_lat = random_a_0_latitudes(-1.4707, -1.4710, len(submission))\n",
    "A1_long = random_a_0_latitudes(37.0657, 37.0659, len(submission))\n",
    "A2_lat = random_a_0_latitudes(-2.2664, -2.2666, len(submission))\n",
    "A2_long = random_a_0_latitudes(37.3877, 37.3885, len(submission))\n",
    "A3_lat = random_a_0_latitudes(-1.0360, -1.0365, len(submission))\n",
    "A3_long = random_a_0_latitudes(37.0601, 37.0621, len(submission))\n",
    "A4_lat = random_a_0_latitudes(-1.2979, -1.2981, len(submission))\n",
    "A4_long = random_a_0_latitudes(36.8372, 36.8375, len(submission))\n",
    "A5_lat = random_a_0_latitudes(-1.2175, -1.2178, len(submission))\n",
    "A5_long = random_a_0_latitudes(36.8961, 36.8965,  len(submission))\n",
    "submission['A0'+'_Latitude'] = A0_lat\n",
    "submission['A0'+'_Longitude'] = A0_long\n",
    "submission['A1'+'_Latitude'] = A1_lat\n",
    "submission['A1'+'_Longitude'] = A1_long\n",
    "submission['A2'+'_Latitude'] = A2_lat\n",
    "submission['A2'+'_Longitude'] = A2_long\n",
    "submission['A3'+'_Latitude'] = A3_lat\n",
    "submission['A3'+'_Longitude'] = A3_long\n",
    "submission['A4'+'_Latitude'] = A4_lat\n",
    "submission['A4'+'_Longitude'] = A4_long\n",
    "submission['A5'+'_Latitude'] = A5_lat\n",
    "submission['A5'+'_Longitude'] = A5_long\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission_file1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambulance_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambulance_locations = ambulance_locations.detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.date_range('2019-01-01', '2020-01-01', freq='3h')\n",
    "sub = pd.DataFrame({\n",
    "    'date':dates\n",
    "})\n",
    "sub['A0'+'_Latitude'] = ambulance_locations[0,0]\n",
    "sub['A0'+'_Longitude'] = ambulance_locations[0][1]\n",
    "sub['A1'+'_Latitude'] = ambulance_locations[1][0]\n",
    "sub['A1'+'_Longitude'] = ambulance_locations[1][1]\n",
    "sub['A2'+'_Latitude'] = ambulance_locations[2][0]\n",
    "sub['A2'+'_Longitude'] = ambulance_locations[2][1]\n",
    "sub['A3'+'_Latitude'] = ambulance_locations[3][0]\n",
    "sub['A3'+'_Longitude'] = ambulance_locations[3][1]\n",
    "sub['A4'+'_Latitude'] = ambulance_locations[4][0]\n",
    "sub['A4'+'_Longitude'] = ambulance_locations[4][1]\n",
    "sub['A5'+'_Latitude'] = ambulance_locations[5][0]\n",
    "sub['A5'+'_Longitude'] = ambulance_locations[5][1]\n",
    "'''for ambulance in range(1, 6):\n",
    "    sub['A'+str(ambulance)+'_Latitude'] = ambulance_locationa[0][0]\n",
    "    sub['A'+str(ambulance)+'_Longitude'] = 0\n",
    "\n",
    "# Place an ambulance in the center of the city:\n",
    "sub['A'+str(ambulance)+'_Latitude'] = 36.82\n",
    "sub['A'+str(ambulance)+'_Longitude'] = -1.3\n",
    "'''\n",
    "\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
